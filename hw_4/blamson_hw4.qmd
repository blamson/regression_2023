---
title: "Homework 4"
author: "Brady Lamson"
format: pdf
editor: visual
---

```{r}
gpa <- read.table("../datasets/gpa.txt", header = T)
```

# Problem 4.14

## Part A

```{r}
no_int_lm <- lm(gpa ~ 0 + act.score, data = gpa)
no_int_lm |> summary()
```

$$Y = 0 - 3.0276X_1 + \epsilon$$

## Part B

```{r}
conf <- confint(no_int_lm, level = .95)
glue::glue(
    "With 95% confidence, the slope of beta_1 is in  
    ({conf[1] |> round(3)}, {conf[2] |> round(3)})
    "
)
```

As the confidence interval does not contain 0, there is significant evidence to indicate that, in the no intercept model, there is a positive relationship between act scores and college freshman gpa.

## Part C

```{r}
my.new.data <- data.frame(act.score = 30)
predict(no_int_lm, newdata = my.new.data, interval = "confidence",
level = 0.95)
```

# Problem 4.15

## Part A

```{r}
plot(
    x = gpa$act.score, y = gpa$gpa, pch=20, 
    xlim = c(0,35), ylim = c(0,4)
)
abline(no_int_lm)

```

The linear regression though the origin does not appear to be a good fit.

## Part B

```{r}
sum_resid <- no_int_lm$residuals |> sum()

glue::glue("The sum of the residuals is {sum_resid |> round(3)}")
```

The residuals do not sum to 0.

```{r}
plot(no_int_lm$fitted.values, no_int_lm$residuals, pch=20)
```

The residuals seem to trend downward as the fitted values increase.

## Part C

```{r}
full_reg <- lm(gpa ~ act.score, data = gpa)

anova(no_int_lm, full_reg)
```

$$
\begin{aligned}
H_0: E(Y) &= \beta_0 + \beta_1 X \\
H_a: E(Y) &\neq \beta_0 + \beta_1 X \\
\alpha &= 0.005 \\ 
F &= 43.401 \\
p &\approx 1.304 \cdot 10^{-9}
\end{aligned}
$$

As $p < \alpha$, there is significant evidence to indicate the linear regression model is not sufficient to explain the relationship in the data.

# Problem 5.5

```{r}
finance <- read.table("../datasets/CH05PR05.txt", header = F)
colnames(finance) <- c("delinquent_loans", "num_companies")
fin_reg <- lm(delinquent_loans ~ num_companies, data = finance)
X <- model.matrix(fin_reg)
Y <- finance$delinquent_loans
```

## Part 1

```{r}
t(Y) %*% Y
```

## Part 2

```{r}
t(X) %*% X
```

```{r}
t(X) %*% Y
```

# Problem 5.13

```{r}
solve(t(X) %*% X)
```

# Problem 5.24

## Part A

```{r}
b <- solve(t(X) %*% X) %*% t(X) %*% Y
fitted_vec <- X %*% b
resid_vec <- Y - (X %*% b)

print("---[Part 1]---")
print("The vector of estimated regression coefficients is")
b
print("---[Part 2]---")
print("The vector of residuals is")
resid_vec
```

## Part C

```{r}
H <- X %*% solve(t(X) %*% X) %*% t(X)
print("The hat matrix is")
print(H)
```

# Problem 6.2
## Part A

$$ 
\textbf{X} = \begin{bmatrix}
1 & X_{11} & X_{12} & X_{11}^2 \\
1 & X_{21} & X_{22} & X_{21}^2 \\
1 & X_{31} & X_{32} & X_{31}^2 \\
1 & X_{41} & X_{42} & X_{41}^2 \\
1 & X_{51} & X_{52} & X_{51}^2 
\end{bmatrix}
$$

$$
\beta = \begin{bmatrix} 
\beta_1\\\beta_2\\\beta_3\\
\end{bmatrix}
$$

## Part B

$$ 
\textbf{X} = \begin{bmatrix}
1 & X_{11} & logX_{12} \\
1 & X_{21} & logX_{22} \\
1 & X_{31} & logX_{32} \\
1 & X_{41} & logX_{42} \\
1 & X_{51} & logX_{52} 
\end{bmatrix}
$$

$$
\beta = \begin{bmatrix} 
\beta_0\\\beta_1\\\beta_2\\
\end{bmatrix}
$$

# Problem 6.3

There are many reasons to potentially ignore insignificant predictors. The first is that $R^2$ is only one measure of the fit of a model and does not its utility. A high $R^2$ on its own is not enough to state that the model is a good fit. As well, models tend to become less useful as you use them to predict values outside of their scope. What this means is that as you add predictors, you add more dimensionality and a wider range of values you likely shouldn't be predicting on. Not only that, more predictors can hurt interpretation of a model. This cost may be worth it if the predictors are good, but if they add little to the predictive power of the model this can easily snowball into black box territory. 

# Problem 6.5

```{r}
brands <- read.table("../datasets/CH06PR05.txt")
colnames(brands) <- c("brand_like", "moisture", "sweetness")
```

# Part A

```{r}
pairs(brands)
```

```{r}
cor(brands)
```

What we can see from this is that brand_like and moisture seem to have a relationship with eachother and are highly correlated. 

Sweetness meanwhile has what seems to be a very non-linear relationship with brand like and less correlation, and is totally uncorrelated to moisture. 

It's worth examining if sweetness is work keeping in the model.

## Part B

```{r}
brand_reg <-lm(brand_like ~ moisture + sweetness, data = brands)
brand_reg |> summary()
```

$$
Y_i = 37.65 + 4.43X_1 + 4.37X_2
$$

$\beta_1$ represents the change in brand likeness as moisture increases. 

## Part C

```{r}
boxplot(brand_reg$residuals)
```


The residuals appear symmetric around 0 and don't seem to do anything weird in either direction. This is a good thing and may give us confidence that the residuals are normally distributed. 

## Part D

```{r}
plot(x = brand_reg$fitted.values, y = brand_reg$residuals, pch=20)
plot(x = brands$moisture, y = brand_reg$residuals, pch=20)
plot(x = brands$sweetness, y = brand_reg$residuals, pch=20)
qqnorm(brand_reg$residuals, pch=20)
qqline(brand_reg$residuals)
```

The residuals seem mostly constant when plot against the fitted values. There's a weird dip at the far left and right of the plot though which may be cause for concern.

The residuals plotted against moisture appear constant, nothing of note here.

The residuals plotted against sweetness seem constant as well, though its harder to gauge as there's only 2 values used in this data.

The normal probability plot approximately follows the line as well which is good. 

# Problem 6.6 

## Part A and B

For this we can refer to the summary output at the start of problem 6. 

$$
H_0: \beta_1 = \beta_2 = 0 \\
H_a: \beta_1 \; or \; \beta_2 \neq 0 \\
\alpha = 0.01 \\
F = 129.1 \\
p = 2.01 \cdot 10^{-5}
$$

As $p < \alpha$, there is significant evidence to indicate that brand like is influenced by moisture and sweetness. This means either $\beta_1$ or $\beta_2$ isn't 0. 

# problem 6.7

## Part A

$$R^2 = .9521$$

This means that 95% of the variation in the data is explained by the regression model.

## Part B

```{r}
ssto <- sum(
    (brands$brand_like - mean(brands$brand_like))^2
)

ssr <- sum(
    (brand_reg$fitted.values - mean(brands$brand_like))^2
)

ssr / ssto
```

The simple and multiple determination coefficient are the same.

# Problem 6.8 

```{r}
new_data = data.frame(moisture = 5, sweetness = 4)
```

## Part A

```{r}
predict(brand_reg, newdata = new_data, interval = "confidence", level = .99)
```

## Part B

```{r}
predict(brand_reg, newdata = new_data, interval = "prediction", level = .99)
```

